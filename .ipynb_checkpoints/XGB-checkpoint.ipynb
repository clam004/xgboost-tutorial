{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost models represent all problems as a regression predictive modeling problem that takes numerical values as input. Your data must be prepared into the expected format:\n",
    "\n",
    "- encode string output variables for classification.\n",
    "- prepare categorical input variables (ie one hot encoding)\n",
    "- handle missing data with XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Case: numeric inputs with categorical outputs\n",
    "\n",
    "meaning of input numbers in the iris dataset\n",
    "\n",
    "   1. sepal length in cm\n",
    "   2. sepal width in cm\n",
    "   3. petal length in cm\n",
    "   4. petal width in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import xgboost\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# load data from http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data \n",
    "data = pandas.read_csv('../data/iris.data', header=None)\n",
    "print(type(data))\n",
    "dataset = data.values # convert pandas dataframe to <class 'numpy.ndarray'>\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.0 2.0 3.5 1.0]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.9 3.2 5.7 2.3]]\n",
      "['Iris-setosa' 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# print every 30th data point\n",
    "print(type(X), X.shape)\n",
    "print(X[::30])\n",
    "print(Y[::30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "print(label_encoded_y[::30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling new unseen categories\n",
    "\n",
    "If you use this label_encoder to convert categories to integer labels for the input to the\n",
    "model, if at test time you encounter a category that is not seen in the training set, the label_encoder can crash, in the future, you may need to custom design a 2 way dictionary\n",
    "that can handle the mapping of number:category that handles new unseen categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected shape of X and Y (150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "print(\"expected shape of X and Y\", X.shape, label_encoded_y.shape)\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n",
    "                                                                    label_encoded_y, \n",
    "                                                                    test_size=test_size, \n",
    "                                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 2 1 2 2 0 2 0]\n",
      "[2, 0, 2, 2, 1, 2, 2, 0, 2, 0]\n",
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred[::5])\n",
    "predictions = [round(value) for value in y_pred]\n",
    "print(predictions[::5])\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Importance is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure may be the purity (Gini index) used to select the split points or another more specific error function.\n",
    "\n",
    "The feature importances are then averaged across all of the the decision trees within the model.\n",
    "\n",
    "For more technical information on how feature importance is calculated in boosted decision trees, see Section 10.13.1 “Relative Importance of Predictor Variables” of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting feature importance is a builtin function of xgboost\n",
    "\n",
    "f0: sepal length in cm\n",
    "\n",
    "f1: sepal width in cm\n",
    "\n",
    "f2: petal length in cm\n",
    "\n",
    "f3: petal width in cm\n",
    "\n",
    "we see tha petal length in cm, f2, is the most important feature, this makes sense, it is the most linearly separable feature as well\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Iris_dataset_scatterplot.svg/800px-Iris_dataset_scatterplot.svg.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "# plot feature importance\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical inputs to Categorical ouputs\n",
    "\n",
    "input features of Breast Cancer Data Set\n",
    "\n",
    "1. Class: no-recurrence-events, recurrence-events\n",
    "2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.\n",
    "3. menopause: lt40, ge40, premeno.\n",
    "4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59\n",
    "5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39\n",
    "6. node-caps: yes, no.\n",
    "7. deg-malig: 1, 2, 3.\n",
    "8. breast: left, right.\n",
    "9. breast-quad: left-up, left-low, right-up,\tright-low, central.\n",
    "10. irradiat:\tyes, no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (286, 9)\n",
      "['no-recurrence-events' '30-39' 'premeno' '30-34' '0-2' 'no' 3 'left'\n",
      " 'left_low']\n",
      "['no-recurrence-events' '30-39' 'premeno' '30-34' '0-2' 'no' '3' 'left'\n",
      " 'left_low']\n",
      "['no' 'no' 'no' 'no' 'no' 'no']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pandas.read_csv('../data/breast-cancer.data', header=None)\n",
    "dataset = data.values\n",
    "\n",
    "# split data into X and y\n",
    "X = dataset[:,0:9]\n",
    "Y = dataset[:,9]\n",
    "\n",
    "print(type(X), X.shape)\n",
    "print(X[0])\n",
    "X = X.astype(str) # deg-malig: int -> str\n",
    "\n",
    "\n",
    "print(X[0])\n",
    "print(Y[::50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost may assume that encoded integer values for each input variable have an ordinal relationship. For example that ‘left-up’ encoded as 0 and ‘left-low’ encoded as 1 for the breast-quad variable have a meaningful relationship as integers. In this case, this assumption is untrue.\n",
    "\n",
    "Instead, we must map these integer values onto new binary variables, one new variable for each categorical value.\n",
    "\n",
    "left-up, left-low, right-up, right-low, central ->\n",
    "\n",
    "1,0,0,0,0\n",
    "\n",
    "0,1,0,0,0\n",
    "\n",
    "0,0,1,0,0\n",
    "\n",
    "0,0,0,1,0\n",
    "\n",
    "0,0,0,0,1\n",
    "\n",
    "using the OneHotEncoder class in scikit-learn\n",
    "\n",
    "We can one hot encode each feature after we have label encoded it. First we must transform the feature array into a 2-dimensional NumPy array where each integer value is a feature vector with a length 1. We can then create the OneHotEncoder and encode the feature array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no-recurrence-events', 'recurrence-events'}\n",
      "{0, 1} (286,)\n",
      "[[0]\n",
      " [0]] (286, 1)\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# what are all the unique categories of the first feature?\n",
    "print(set(X[:,0]))\n",
    "label_encoder = LabelEncoder()\n",
    "feature = label_encoder.fit_transform(X[:,0])\n",
    "# they have now be encoded ordinally, but they are not ordinal\n",
    "print(set(feature), feature.shape)\n",
    "feature = feature.reshape(X.shape[0], 1) # add dimension: (286,) -> (286, 1) \n",
    "print(feature[:2], feature.shape)\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "feature = onehot_encoder.fit_transform(feature)\n",
    "print(feature[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: :  (286, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoded_x = None\n",
    "for i in range(0, X.shape[1]):\n",
    "    label_encoder = LabelEncoder()\n",
    "    feature = label_encoder.fit_transform(X[:,i])\n",
    "    feature = feature.reshape(X.shape[0], 1)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    if encoded_x is None:\n",
    "        encoded_x = feature\n",
    "    else:\n",
    "        encoded_x = numpy.concatenate((encoded_x, feature), axis=1)\n",
    "        \n",
    "print(\"X shape: : \", encoded_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_x[::90] # The input consists of each features one hot encoding concatenated together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# encode string class values as integers\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)\n",
    "print(label_encoded_y[::90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(encoded_x, \n",
    "                                                    label_encoded_y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
      "              validate_parameters=False, verbosity=None)\n",
      "Accuracy: 71.58%\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Missing Data \n",
    "\n",
    "XGBoost was designed to work with sparse data, like the one hot encoded data from the previous section, and missing data is handled the same way that sparse or zero values are handled, by minimizing the loss function. Section 3.4 “Sparsity-aware Split Finding” in the paper [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)\n",
    "\n",
    "The Horse Colic dataset is a good example to demonstrate this capability as it contains a large percentage of missing data, approximately 30%.\n",
    "\n",
    "1: surgery?\n",
    "1 = Yes, it had surgery\n",
    "2 = It was treated without surgery\n",
    "\n",
    "2: Age\n",
    "1 = Adult horse\n",
    "2 = Young (< 6 months)\n",
    "\n",
    "3: Hospital Number\n",
    "- numeric id\n",
    "- the case number assigned to the horse (may not be unique if the horse is treated > 1 time)\n",
    "\n",
    "4: rectal temperature\n",
    "- linear\n",
    "- in degrees celsius.\n",
    "- An elevated temp may occur due to infection.\n",
    "- temperature may be reduced when the animal is in late shock\n",
    "- normal temp is 37.8\n",
    "- this parameter will usually change as the problem progresses, eg. may start out normal, then become elevated because of the lesion, passing back through the normal range as the horse goes into shock\n",
    "\n",
    "21: abdominocentesis appearance\n",
    "- a needle is put in the horse's abdomen and fluid is obtained from\n",
    "the abdominal cavity\n",
    "- possible values:\n",
    "1 = clear\n",
    "2 = cloudy\n",
    "3 = serosanguinous\n",
    "- normal fluid is clear while cloudy or serosanguinous indicates a compromised gut\n",
    "\n",
    "22: abdomcentesis total protein\n",
    "- linear\n",
    "- the higher the level of protein the more likely it is to have a compromised gut. Values are in gms/dL\n",
    "\n",
    "23: outcome\n",
    "- what eventually happened to the horse?\n",
    "- possible values:\n",
    "1 = lived\n",
    "2 = died\n",
    "3 = was euthanized\n",
    "\n",
    "24: surgical lesion?\n",
    "- retrospectively, was the problem (lesion) surgical?\n",
    "- all cases are either operated upon or autopsied so that this value and the lesion type are always known\n",
    "- possible values:\n",
    "1 = Yes\n",
    "2 = No\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num features:  28\n",
      "[['2' 1 530101 '38.50' '66' '28' '3' '3' '?' '2' '5' '4' '4' '?' '?' '?'\n",
      "  '3' '5' '45.00' '8.40' '?' '?' '2' 2 11300 0 0 2]\n",
      " ['2' 1 529461 '40.30' '114' '36' '3' '3' '1' '2' '2' '3' '3' '2' '1'\n",
      "  '7.00' '1' '5' '57.00' '8.10' '3' '4.50' '2' 1 3205 0 0 1]\n",
      " ['1' 1 5279822 '38.00' '?' '24' '3' '3' '6' '2' '5' '?' '4' '1' '1' '?'\n",
      "  '?' '?' '68.00' '7.80' '?' '?' '2' 1 2205 0 0 2]\n",
      " ['1' 1 528638 '37.70' '120' '28' '3' '3' '3' '1' '5' '3' '3' '1' '1' '?'\n",
      "  '?' '?' '65.00' '7.00' '3' '?' '2' 1 4205 0 0 1]]\n",
      "{0, 2209}\n",
      "{0, 6112, 3111, 7111, 3112, 1400}\n",
      "{0, 2305, 5124, 3205, 31110, 3207, 5000, 3209, 11400, 4111, 400, 2322, 41110, 5400, 4122, 4124, 2205, 2206, 2207, 2208, 2209, 11300, 3111, 3112, 7209, 9000, 3115, 300, 3113, 12208, 3124, 9400, 3133, 2111, 2112, 2113, 6209, 7111, 3400, 7113, 4300, 2124, 8400, 3025, 5205, 5206, 1111, 6111, 6112, 1124, 3300, 7400, 8300, 4205, 4206, 4207, 11124, 21110, 5111, 1400, 2300}\n",
      "{1, 2}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataframe = pandas.read_csv(\"../data/horse-colic.data\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "print(\" num features: \", len(dataset[0]))\n",
    "print(dataset[::91])\n",
    "# these features are kinda weird, they can be 0 or large integers\n",
    "print(set(dataset[:,26]))\n",
    "print(set(dataset[:,25]))\n",
    "print(set(dataset[:,24]))\n",
    "# the last feature is binary \n",
    "print(set(dataset[:,27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (300, 27)\n",
      "['2' 1 530101 '38.50' '66' '28' '3' '3' '?' '2' '5' '4' '4' '?' '?' '?'\n",
      " '3' '5' '45.00' '8.40' '?' '?' '2' 2 11300 0 0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# split data into X and y\n",
    "X = dataset[:,0:27]\n",
    "Y = dataset[:,27]\n",
    "# print every 30th data point\n",
    "print(type(X), X.shape)\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data is marked with a question mark character (‘?’). We can change these missing values to the sparse value expected by XGBoost which is the value zero (0).\n",
    "\n",
    "Because the missing data was marked as strings, those columns with missing data were all loaded as string data types. We can now convert the entire set of input data to numerical values.\n",
    "\n",
    "For this dataset, the numeric data start at 1 and not 0, so zeros do not occur unless inthe case there is no data such as in features 25,26,27. There are 2 ways to handle the missing data:\n",
    "\n",
    "You can assign the mean value to them\n",
    "\n",
    "`imputer = Imputer()`\n",
    "`X = imputer.fit_transform(X)`\n",
    "\n",
    "or you can assign them to 0\n",
    "\n",
    "`X[X == '?'] = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (300, 27)\n",
      "[2.00000e+00 1.00000e+00 5.30101e+05 3.85000e+01 6.60000e+01 2.80000e+01\n",
      " 3.00000e+00 3.00000e+00 0.00000e+00 2.00000e+00 5.00000e+00 4.00000e+00\n",
      " 4.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 3.00000e+00 5.00000e+00\n",
      " 4.50000e+01 8.40000e+00 0.00000e+00 0.00000e+00 2.00000e+00 2.00000e+00\n",
      " 1.13000e+04 0.00000e+00 0.00000e+00]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# set missing values to 0\n",
    "X[X == '?'] = 0\n",
    "# convert to numeric\n",
    "X = X.astype('float32')\n",
    "\n",
    "print(type(X), X.shape)\n",
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Y class values as integers 0, 1 instead of 1, 2\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 27) (300,)\n",
      "[2.00000e+00 1.00000e+00 5.30101e+05 3.85000e+01 6.60000e+01 2.80000e+01\n",
      " 3.00000e+00 3.00000e+00 0.00000e+00 2.00000e+00 5.00000e+00 4.00000e+00\n",
      " 4.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 3.00000e+00 5.00000e+00\n",
      " 4.50000e+01 8.40000e+00 0.00000e+00 0.00000e+00 2.00000e+00 2.00000e+00\n",
      " 1.13000e+04 0.00000e+00 0.00000e+00]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "print(X.shape, label_encoded_y.shape)\n",
    "print(X[0])\n",
    "print(label_encoded_y[0])\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n",
    "                                                    label_encoded_y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
      "              validate_parameters=False, verbosity=None)\n",
      "Accuracy: 82.83%\n"
     ]
    }
   ],
   "source": [
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection: Pima Indians Diabetes Database\n",
    "\n",
    "The diagnostic, binary-valued variable investigated is whether the\n",
    "patient shows signs of diabetes according to World Health Organization\n",
    "criteria (i.e., if the 2 hour post-load plasma glucose was at least \n",
    "200 mg/dl at any survey  examination or if found during routine medical\n",
    "care).   The population lives near Phoenix, Arizona, USA.\n",
    "\n",
    "Results: Their ADAP algorithm makes a real-valued prediction between\n",
    "0 and 1.  This was transformed into a binary decision using a cutoff of \n",
    "0.448.  Using 576 training instances, the sensitivity and specificity\n",
    "of their algorithm was 76% on the remaining 192 instances.\n",
    "\n",
    "5. Number of Instances: 768\n",
    "\n",
    "6. Number of Attributes: 8 plus class \n",
    "\n",
    "7. For Each Attribute: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function (represents ancestor’s history)\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)\n",
    "\n",
    "8. Missing Attribute Values: Yes\n",
    "\n",
    "9. Class Distribution: (class value 1 is interpreted as \"tested positive for\n",
    "   diabetes\")\n",
    "\n",
    "   Class Value  Number of instances\n",
    "   \n",
    "   0            500\n",
    "   \n",
    "   1            268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority classifier 0.651\n",
      "[[6.000e+00 1.480e+02 7.200e+01 3.500e+01 0.000e+00 3.360e+01 6.270e-01\n",
      "  5.000e+01 1.000e+00]\n",
      " [1.000e+00 1.630e+02 7.200e+01 0.000e+00 0.000e+00 3.900e+01 1.222e+00\n",
      "  3.300e+01 1.000e+00]\n",
      " [0.000e+00 1.130e+02 8.000e+01 1.600e+01 0.000e+00 3.100e+01 8.740e-01\n",
      "  2.100e+01 0.000e+00]\n",
      " [0.000e+00 1.670e+02 0.000e+00 0.000e+00 0.000e+00 3.230e+01 8.390e-01\n",
      "  3.000e+01 1.000e+00]\n",
      " [4.000e+00 9.500e+01 6.400e+01 0.000e+00 0.000e+00 3.200e+01 1.610e-01\n",
      "  3.100e+01 1.000e+00]\n",
      " [2.000e+00 1.170e+02 9.000e+01 1.900e+01 7.100e+01 2.520e+01 3.130e-01\n",
      "  2.100e+01 0.000e+00]\n",
      " [1.000e+00 1.080e+02 8.800e+01 1.900e+01 0.000e+00 2.710e+01 4.000e-01\n",
      "  2.400e+01 0.000e+00]\n",
      " [2.000e+00 1.220e+02 7.600e+01 2.700e+01 2.000e+02 3.590e+01 4.830e-01\n",
      "  2.600e+01 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = numpy.loadtxt('../data/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "#dataframe = pandas.read_csv('../data/pima-indians-diabetes.csv', delimiter=\",\")\n",
    "#dataset = dataframe.values\n",
    "print(\"majority classifier\", round(500/(500+268),3))\n",
    "print(dataset[::100])\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 1.000e+00, 3.360e+01,\n",
       "        6.270e-01, 5.000e+01],\n",
       "       [1.000e+00, 1.630e+02, 7.200e+01, 1.000e+00, 1.000e+00, 3.900e+01,\n",
       "        1.222e+00, 3.300e+01],\n",
       "       [0.000e+00, 1.130e+02, 8.000e+01, 1.600e+01, 1.000e+00, 3.100e+01,\n",
       "        8.740e-01, 2.100e+01],\n",
       "       [0.000e+00, 1.670e+02, 0.000e+00, 1.000e+00, 1.000e+00, 3.230e+01,\n",
       "        8.390e-01, 3.000e+01],\n",
       "       [4.000e+00, 9.500e+01, 6.400e+01, 1.000e+00, 1.000e+00, 3.200e+01,\n",
       "        1.610e-01, 3.100e+01],\n",
       "       [2.000e+00, 1.170e+02, 9.000e+01, 1.900e+01, 7.100e+01, 2.520e+01,\n",
       "        3.130e-01, 2.100e+01],\n",
       "       [1.000e+00, 1.080e+02, 8.800e+01, 1.900e+01, 1.000e+00, 2.710e+01,\n",
       "        4.000e-01, 2.400e+01],\n",
       "       [2.000e+00, 1.220e+02, 7.600e+01, 2.700e+01, 2.000e+02, 3.590e+01,\n",
       "        4.830e-01, 2.600e+01]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,3][X[:,3]==0.]=numpy.around(numpy.mean(X[:,3]!=0.))\n",
    "X[:,4][X[:,4]==0.]=numpy.around(numpy.mean(X[:,4]!=0.))\n",
    "X[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.95%\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n",
    "                                                                    Y, \n",
    "                                                                    test_size=0.33, \n",
    "                                                                    random_state=1)\n",
    "# fit model on all training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.feature_importances_ [0.10440847 0.21552509 0.09496948 0.08279032 0.106089   0.15713796\n",
      " 0.09531099 0.14376874]\n",
      "majority classifier 0.6377952755905512\n",
      "num_features=1, Accuracy: 70.47%\n",
      "num_features=2, Accuracy: 70.87%\n",
      "num_features=3, Accuracy: 74.80%\n",
      "num_features=4, Accuracy: 72.83%\n",
      "num_features=5, Accuracy: 77.17%\n",
      "num_features=6, Accuracy: 76.38%\n",
      "num_features=7, Accuracy: 77.17%\n",
      "num_features=8, Accuracy: 75.59%\n"
     ]
    }
   ],
   "source": [
    "# Fit model using each importance as a threshold\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "print(\"model.feature_importances_\", model.feature_importances_)\n",
    "print(\"majority classifier\",1 - numpy.mean(y_test))\n",
    "sort_idx = numpy.argsort(-model.feature_importances_) # sort in decreasing order\n",
    "best_feature = sort_idx[0]  \n",
    "X_train_build = X_train[:,best_feature].reshape(X_train.shape[0], 1)\n",
    "X_test_build = X_test[:,best_feature].reshape(X_test.shape[0], 1)\n",
    "for idx in list(sort_idx):\n",
    "    if idx != best_feature:\n",
    "        X_train_build = numpy.concatenate((X_train_build, \n",
    "                                           X_train[:,idx].reshape(X_train.shape[0], 1)), \n",
    "                                           axis=1)\n",
    "        X_test_build = numpy.concatenate((X_test_build, \n",
    "                                          X_test[:,idx].reshape(X_test.shape[0], 1)), \n",
    "                                          axis=1)\n",
    "    # train model\n",
    "    selection_model = xgboost.XGBClassifier()\n",
    "    selection_model.fit(X_train_build, y_train)\n",
    "    # eval model\n",
    "    y_pred = selection_model.predict(X_test_build)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"num_features=%d, Accuracy: %.2f%%\" % ( X_train_build.shape[1], \n",
    "                                                   accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
